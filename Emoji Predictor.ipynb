{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c50e032-3b13-4104-9058-ed3d4667926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Activation\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c73ee96-2a46-47a7-9354-7994e0caaf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset for Training \n",
    "train_data=pd.read_excel(\"Training_data.xlsx\")\n",
    "#dataset for Testing\n",
    "test_data=pd.read_excel(\"Testing_data.xlsx\")\n",
    "\n",
    "train_df = pd.DataFrame(train_data, columns=[\"Sentence\", \"Label\"])\n",
    "test_df = pd.DataFrame(test_data, columns=[\"Sentence\", \"Label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0de584-8a46-41d5-a352-1f946a50e3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the training data:\n",
      "                  Sentence  Label\n",
      "0         I lost my wallet      3\n",
      "1   You failed the midterm      3\n",
      "2  Congrats on the new job      2\n",
      "3            I want to eat      4\n",
      "4       That catcher sucks      1\n",
      "First few rows of the testing data:\n",
      "                   Sentence  Label\n",
      "0         he did not answer      3\n",
      "1      she got me a present      0\n",
      "2  ha ha ha it was so funny      2\n",
      "3       he is a good friend      0\n",
      "4                I am upset      3\n",
      "\n",
      "Basic statistics of the training data:\n",
      "           Label\n",
      "count  27.000000\n",
      "mean    2.037037\n",
      "std     1.255189\n",
      "min     0.000000\n",
      "25%     1.500000\n",
      "50%     2.000000\n",
      "75%     3.000000\n",
      "max     4.000000\n",
      "\n",
      "Basic statistics of the testing data:\n",
      "          Label\n",
      "count  8.000000\n",
      "mean   1.750000\n",
      "std    1.581139\n",
      "min    0.000000\n",
      "25%    0.000000\n",
      "50%    2.000000\n",
      "75%    3.000000\n",
      "max    4.000000\n",
      "\n",
      "Information about the training data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27 entries, 0 to 26\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  27 non-null     object\n",
      " 1   Label     27 non-null     int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 564.0+ bytes\n",
      "None\n",
      "\n",
      "Information about the testing data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8 entries, 0 to 7\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  8 non-null      object\n",
      " 1   Label     8 non-null      int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 260.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Exploratory Data Analysis (EDA) Performance\n",
    "# Display the first few rows of the training data\n",
    "print(\"First few rows of the training data:\")\n",
    "print(train_df.head()) \n",
    "\n",
    "# Display the first few rows of the testing data\n",
    "print(\"First few rows of the testing data:\")\n",
    "print(test_df.head())  \n",
    "# Display basic statistics of the training data\n",
    "print(\"\\nBasic statistics of the training data:\")\n",
    "print(train_df.describe()) \n",
    "\n",
    "# Display basic statistics of the testing data\n",
    "print(\"\\nBasic statistics of the testing data:\")\n",
    "print(test_df.describe())  \n",
    "\n",
    "# Display information about the training data\n",
    "print(\"\\nInformation about the training data:\")\n",
    "print(train_df.info()) \n",
    "\n",
    "# Display information about the testing data\n",
    "print(\"\\nInformation about the testing data:\")\n",
    "print(test_df.info())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b98799f-d5c6-4350-a888-a99a5d7c42c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "class data_cleaning:\n",
    "    #function to remove duplicates\n",
    "    def remove_duplicates(self,data):        \n",
    "        data.drop_duplicates(inplace=True)\n",
    "    #function to handle missing values\n",
    "    def handling_empty_cells(self,data):\n",
    "        data.dropna(inplace=True)\n",
    "obj=data_cleaning()\n",
    "obj.remove_duplicates(train_df)\n",
    "obj.handling_empty_cells(train_df)\n",
    "obj.remove_duplicates(test_df)\n",
    "obj.handling_empty_cells(test_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db94f1a4-3697-4ed9-afbb-45e5028bdb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing\n",
      "{'i': 1, 'you': 2, 'ha': 3, 'the': 4, 'this': 5, 'a': 6, 'is': 7, 'not': 8, 'joke': 9, 'am': 10, 'got': 11, 'he': 12, 'my': 13, 'for': 14, 'was': 15, 'such': 16, 'good': 17, 'it': 18, 'so': 19, 'happy': 20, 'did': 21, 'she': 22, 'lost': 23, 'wallet': 24, 'failed': 25, 'midterm': 26, 'congrats': 27, 'on': 28, 'new': 29, 'job': 30, 'want': 31, 'to': 32, 'eat': 33, 'that': 34, 'catcher': 35, 'sucks': 36, 'are': 37, 'qualified': 38, 'position': 39, 'love': 40, 'dad': 41, 'guy': 42, 'specialization': 43, 'great': 44, 'could': 45, 'solve': 46, 'proud': 47, 'of': 48, 'forever': 49, 'first': 50, 'baseman': 51, 'ball': 52, 'bad': 53, 'do': 54, 'your': 55, 'homework': 56, 'answer': 57, 'raise': 58, 'me': 59, 'present': 60, 'funny': 61, 'friend': 62, 'upset': 63, 'we': 64, 'had': 65, 'lovely': 66, 'dinner': 67, 'tonight': 68, 'where': 69, 'food': 70, 'stop': 71, 'making': 72}\n",
      "Sequencing x_train\n",
      "[[1, 23, 13, 24], [2, 25, 4, 26], [27, 28, 4, 29, 30], [1, 31, 32, 33], [34, 35, 36], [2, 37, 8, 38, 14, 5, 39], [1, 40, 13, 41], [5, 42, 15, 16, 6, 9], [17, 9], [5, 43, 7, 44], [2, 45, 8, 46, 18], [1, 10, 19, 20, 14, 2], [1, 10, 47, 48, 2, 49], [4, 50, 51, 11, 4, 52], [5, 7, 53], [2, 21, 8, 54, 55, 56], [12, 21, 8, 57], [12, 11, 6, 58], [22, 11, 59, 6, 60], [3, 3, 3, 18, 15, 19, 61], [12, 7, 6, 17, 62], [1, 10, 63], [64, 65, 16, 6, 66, 67, 68], [69, 7, 4, 70], [71, 72, 5, 9, 3, 3, 3], [22, 7, 20]]\n",
      "Sequencing x_test\n",
      "[[12, 21, 8, 57], [22, 11, 59, 6, 60], [3, 3, 3, 18, 15, 19, 61], [12, 7, 6, 17, 62], [1, 10, 63], [64, 65, 16, 6, 66, 67, 68], [69, 7, 4, 70], [71, 72, 5, 9, 3, 3, 3]]\n",
      "Padding\n",
      "Padding x_train\n",
      "[[ 1 23 13 24  0  0  0  0  0  0]\n",
      " [ 2 25  4 26  0  0  0  0  0  0]\n",
      " [27 28  4 29 30  0  0  0  0  0]\n",
      " [ 1 31 32 33  0  0  0  0  0  0]\n",
      " [34 35 36  0  0  0  0  0  0  0]\n",
      " [ 2 37  8 38 14  5 39  0  0  0]\n",
      " [ 1 40 13 41  0  0  0  0  0  0]\n",
      " [ 5 42 15 16  6  9  0  0  0  0]\n",
      " [17  9  0  0  0  0  0  0  0  0]\n",
      " [ 5 43  7 44  0  0  0  0  0  0]\n",
      " [ 2 45  8 46 18  0  0  0  0  0]\n",
      " [ 1 10 19 20 14  2  0  0  0  0]\n",
      " [ 1 10 47 48  2 49  0  0  0  0]\n",
      " [ 4 50 51 11  4 52  0  0  0  0]\n",
      " [ 5  7 53  0  0  0  0  0  0  0]\n",
      " [ 2 21  8 54 55 56  0  0  0  0]\n",
      " [12 21  8 57  0  0  0  0  0  0]\n",
      " [12 11  6 58  0  0  0  0  0  0]\n",
      " [22 11 59  6 60  0  0  0  0  0]\n",
      " [ 3  3  3 18 15 19 61  0  0  0]\n",
      " [12  7  6 17 62  0  0  0  0  0]\n",
      " [ 1 10 63  0  0  0  0  0  0  0]\n",
      " [64 65 16  6 66 67 68  0  0  0]\n",
      " [69  7  4 70  0  0  0  0  0  0]\n",
      " [71 72  5  9  3  3  3  0  0  0]\n",
      " [22  7 20  0  0  0  0  0  0  0]]\n",
      "Padding x_test\n",
      "[[12 21  8 57  0  0  0  0  0  0]\n",
      " [22 11 59  6 60  0  0  0  0  0]\n",
      " [ 3  3  3 18 15 19 61  0  0  0]\n",
      " [12  7  6 17 62  0  0  0  0  0]\n",
      " [ 1 10 63  0  0  0  0  0  0  0]\n",
      " [64 65 16  6 66 67 68  0  0  0]\n",
      " [69  7  4 70  0  0  0  0  0  0]\n",
      " [71 72  5  9  3  3  3  0  0  0]]\n",
      "One-hot encode the labels\n",
      "One-hot encode the  y-train\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "One-hot encode the  y_test\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Text Tokenization, Padding and  Sequencing:\n",
    "max_len = 10  # maximum number of words in a sentence\n",
    "#Tokenizing\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['Sentence'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Tokenizing\")\n",
    "print(word_index)\n",
    "vocab_size = len(word_index) + 1  \n",
    "#Sequencing\n",
    "print(\"Sequencing x_train\")\n",
    "x_train = tokenizer.texts_to_sequences(train_df['Sentence'].values)\n",
    "print(x_train)\n",
    "print(\"Sequencing x_test\")\n",
    "x_test = tokenizer.texts_to_sequences(test_df['Sentence'].values)\n",
    "print(x_test)\n",
    "#Padding\n",
    "print(\"Padding\")\n",
    "x_train = pad_sequences(x_train,padding=\"post\", maxlen=max_len)\n",
    "print(\"Padding x_train\")\n",
    "print(x_train)\n",
    "x_test = pad_sequences(x_test, padding=\"post\",maxlen=max_len)\n",
    "print(\"Padding x_test\")\n",
    "print(x_test)\n",
    "# Verify the number of classes\n",
    "num_classes = len(set(train_df['Label'].values))\n",
    "\n",
    "# One-hot encode the labels\n",
    "print(\"One-hot encode the labels\")\n",
    "y_train = to_categorical(train_df['Label'].values, num_classes=num_classes)\n",
    "print(\"One-hot encode the  y-train\")\n",
    "print(y_train)\n",
    "y_test = to_categorical(test_df['Label'].values, num_classes=num_classes)\n",
    "print(\"One-hot encode the  y_test\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42a2dcad-3a3c-461b-baa9-8874157d59e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.13292     0.16985001 -0.1436     ... -0.23778     0.14766\n",
      "   0.62901998]\n",
      " [-0.28426999  0.047977   -0.15062    ... -0.090071    0.016922\n",
      "   0.29278001]\n",
      " ...\n",
      " [ 0.38543999  0.34246999  0.29598999 ... -0.49944001 -0.27509999\n",
      "  -0.61361003]\n",
      " [ 0.80465001  0.051994    0.078946   ... -0.23231     0.066039\n",
      "   0.1154    ]\n",
      " [ 0.34911001  0.20057     0.2419     ... -0.46020001 -0.31044\n",
      "   0.099268  ]]\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "embeddings_index = {}\n",
    "embedding_dim = 300\n",
    "with open(r'C:\\Users\\user\\Downloads\\glove.6B (1)\\glove.6B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "print(embedding_matrix)\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(embedding_matrix) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "030bb878-8991-4fa7-8860-078f69d8d47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">21,900</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │          \u001b[38;5;34m21,900\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,900</span> (85.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,900\u001b[0m (85.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,900</span> (85.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m21,900\u001b[0m (85.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.3043 - loss: 1.6228 - val_accuracy: 0.3333 - val_loss: 1.6135\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.3043 - loss: 1.5663 - val_accuracy: 0.3333 - val_loss: 1.5916\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.4348 - loss: 1.5135 - val_accuracy: 0.3333 - val_loss: 1.5748\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.5217 - loss: 1.4894 - val_accuracy: 0.3333 - val_loss: 1.5585\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.3913 - loss: 1.4484 - val_accuracy: 0.3333 - val_loss: 1.5446\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.4783 - loss: 1.4459 - val_accuracy: 0.3333 - val_loss: 1.5318\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.4348 - loss: 1.3870 - val_accuracy: 0.3333 - val_loss: 1.5198\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.4783 - loss: 1.3485 - val_accuracy: 0.3333 - val_loss: 1.5105\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.4783 - loss: 1.3465 - val_accuracy: 0.3333 - val_loss: 1.5031\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.5652 - loss: 1.2891 - val_accuracy: 0.6667 - val_loss: 1.4951\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.5217 - loss: 1.2816 - val_accuracy: 0.6667 - val_loss: 1.4872\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.4783 - loss: 1.2447 - val_accuracy: 0.6667 - val_loss: 1.4797\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.6087 - loss: 1.2319 - val_accuracy: 0.6667 - val_loss: 1.4707\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.6087 - loss: 1.1820 - val_accuracy: 0.6667 - val_loss: 1.4644\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.7391 - loss: 1.1317 - val_accuracy: 0.6667 - val_loss: 1.4613\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.5652 - loss: 1.0768 - val_accuracy: 0.6667 - val_loss: 1.4595\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.6957 - loss: 1.0702 - val_accuracy: 0.6667 - val_loss: 1.4563\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.6522 - loss: 1.0096 - val_accuracy: 0.6667 - val_loss: 1.4515\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.7826 - loss: 0.9667 - val_accuracy: 0.6667 - val_loss: 1.4447\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8696 - loss: 0.9122 - val_accuracy: 0.6667 - val_loss: 1.4414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2179a79ce60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    " \n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=20, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b28b9882-e89a-41fb-a8a8-4a077936ee3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7500 - loss: 1.0277\n",
      "Test Accuracy: 75.00%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step\n",
      "Precision: 0.67%\n",
      "Recall: 0.75%\n",
      "F1-score: 0.70%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate other evaluation metrics\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "# Predict classes for test data\n",
    "y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
    "\n",
    "# Calculate precision, recall, and F1-score using sklearn \n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(np.argmax(y_test, axis=-1), y_pred, average='weighted',zero_division=0)\n",
    "\n",
    "\n",
    "print(f'Precision: {precision:.2f}%')\n",
    "print(f'Recall: {recall:.2f}%')\n",
    "print(f'F1-score: {f1_score:.2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "545b599d-948a-4d88-bbab-a3e7787bd3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Sentence: he did not answer| Actual Emoji: 😓 |Predicted Emoji: 😓\n",
      "Sentence: she got me a present| Actual Emoji: 🎁 |Predicted Emoji: 🎁\n",
      "Sentence: ha ha ha it was so funny| Actual Emoji: 😄 |Predicted Emoji: 😄\n",
      "Sentence: he is a good friend| Actual Emoji: 🎁 |Predicted Emoji: 🎁\n",
      "Sentence: I am upset| Actual Emoji: 😓 |Predicted Emoji: 😄\n",
      "Sentence: We had such a lovely dinner tonight| Actual Emoji: 🎁 |Predicted Emoji: 🎁\n",
      "Sentence: where is the food| Actual Emoji: 🍴 |Predicted Emoji: 😓\n",
      "Sentence: Stop making this joke ha ha ha| Actual Emoji: 😄 |Predicted Emoji: 😄\n"
     ]
    }
   ],
   "source": [
    "emoji_dictionary = {\n",
    "    \"0\": \"🎁\",  # Assuming a set of emojis\n",
    "    \"1\": \"😡\",\n",
    "    \"2\": \"😄\",\n",
    "    \"3\": \"😓\",\n",
    "    \"4\": \"🍴\"\n",
    "}\n",
    "\n",
    "pred_probabilities = model.predict(x_test)\n",
    "pred = np.argmax(pred_probabilities, axis=1)\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    sentence = test_df['Sentence'].values[i]\n",
    "    actual_emoji = emoji_dictionary[str(np.argmax(y_test[i]))]\n",
    "    predicted_emoji = emoji_dictionary[str(pred[i])]\n",
    "    print(f'Sentence: {sentence}| Actual Emoji: {actual_emoji} |Predicted Emoji: {predicted_emoji}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a53b00e-62a5-4be6-864a-771b315b651c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f380894d-15a1-4b3d-8e49-35951cd8c38f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485685f-4777-4dad-bc02-e9ee7aae37fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
